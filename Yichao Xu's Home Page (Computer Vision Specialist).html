<!DOCTYPE html>
<!-- templet saved from url=(0033)http://www.ee.columbia.edu/~zhuo/ -->
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="zhuo chen">

    <title>Yichao Xu's Home Page (Computer Vision Specialist)</title>

    <!-- Bootstrap Core CSS -->
    <link href="css/bootstrap.min.css" rel="stylesheet">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="css/style.css">
    <link href="css/scrolling-nav.css" rel="stylesheet">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

<!--<style id="style-1-cropbar-clipper">/* Copyright 2014 Evernote Corporation. All rights reserved. */
.en-markup-crop-options {
    top: 18px !important;
    left: 50% !important;
    margin-left: -100px !important;
    width: 200px !important;
    border: 2px rgba(255,255,255,.38) solid !important;
    border-radius: 4px !important;
}

.en-markup-crop-options div div:first-of-type {
    margin-left: 0px !important;
}
</style>-->

</head>

<!-- The #page-top ID is part of the scrolling feature - the data-spy and data-target are part of the built-in Bootstrap scrollspy function -->

<body id="page-top" data-spy="scroll" data-target=".navbar-fixed-top">

    <!-- Navigation -->
    <nav class="navbar navbar-default navbar-fixed-top top-nav-collapse" id="main-menu" role="navigation">
        <div class="container">
            <div class="navbar-header page-scroll">
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-ex1-collapse">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a class="navbar-brand page-scroll" id="yichao" href="#page-top"><b>Yichao Xu</b></a>
            </div>

            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse navbar-ex1-collapse">
                <ul class="nav navbar-nav">
                    <!-- Hidden li included to remove active class from about link when scrolled up past about section -->
                    <li class="hidden">
                        <a class="page-scroll" href="#intro"></a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#highlight">Highlight</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#education">Education</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#work">Work</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#patent">Patent</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#publication">Publication</a>
                    </li>
                </ul>
            </div>
            <!-- /.navbar-collapse -->
        </div>
        <!-- /.container -->
    </nav>

    <!-- Intro Section -->
    <section id="intro" class="intro-section">
        <div class="container">
            <div class="row">
                <div class="col-lg-6">
                    <img class="photo" src="img/myphoto.jpg" alt="myPhoto">
                </div>
                <div class="col-lg-6">
                    <p id="discription">Yichao Xu is currently a senior engineer at Hong Kong Applied Science and Technology Research Institute (ASTRI).
                        Before joining ASTRI, he has worked as a Postdoctoral Research Fellow at School of Creative Media, City University of Hong Kong in 2016.
                        He has also worked at Department of Advanced Information Technology, Kyushu University, Fukuoka, Japan, as a Research Technician from 2014 to 2016.
                        Yichao earned his Ph.D. degree in information science and technology from Kyushu University in 2015.
                        Prior to Kyushu University, Yichao received his Master degree from University of Chinese Academy of Sciences, China in 2010, and his Bachelor degree from Beijing Electronic Science and Technology Institute, China in 2007.
                        His research interests are computer vision and computational photography.
                    </p>
                    <ul class="contact">
                        <li><img class="icons" src="img/email.png" alt="my_email">xuyc2010(At)gmail.com</li>
                        <!--<li><img class="icons" src="img/telephone.png" alt="my_phone">+1(646)465-1394</li>-->
                    </ul>
                </div>
            </div>
        </div>
    </section>

    <!-- Education Section -->
    <section id="highlight" class="highlight-section">
        <div class="container">
            <h1>HIGHLIGHT</h1>
            <div class="row">
                <h3 class="col-md-10">Deep attractor network</h3>
                <p class="col-md-2" id="year">2016</p>
                <div class="col-md-10">
                    <p>Together with Yi Luo, and Prof. Nima Mesgarani, we made the generalized version of deep clustering, allowing direct end-to-end optimization for multi-speaker separation, see demo  <b><i><a href="http://danetapi.com/danet" target="_blank"> here  </a></i></b> </p>
                </div>
                    
                <h3 class="col-md-10">Deep music separation</h3>
                <p class="col-md-2" id="year">2016</p>     
                <div class="col-md-10">
                    <p>Together with Yi Luo, Nima, Mesgarani, Jonathan Leroux and John Hershey, we largely increase the state of the art performance in music separation with our model, the Chimera network, and won the best performance in MIREX 2016 on singing separation track, see demo <b><i><a href="http://danetapi.com/chimera" target="_blank"> here  </a></i></b> </p>
                </div>

                <h3 class="col-md-10">Neural decoding of attentional selection in multi-speaker separation </h3>
                <p class="col-md-2" id="year">2016</p>     
                <div class="col-md-10">
                    <p>Together with James O'Sullivan, Sameer Sheth, Guy, McKhann; Ashesh Mehta, and  Nima Mesgarani, we create this revolutionary device, that allows the patient to directly separate out the audio targets with high quality, using their attention as guiding clue, which is the next step of the hearing aid industry</p>
                </div>
                    
                <h3 class="col-md-10">Adaptation of neural networks constrained by prior statistics of node co-activations </h3>
                <p class="col-md-2" id="year">2016</p>     
                <div class="col-md-10">
                    <p>Together with Tasha Nagamine and  Nima Mesgarani, we created a unsupervisely model for neural network adaptaion, which largely increase the robustness for ASR system under noisy enviorment</p>
                </div>

                <h3 class="col-md-10">End-to-End Attention based Speaker Verification </h3>
                <p class="col-md-2" id="year">2016</p>     
                <div class="col-md-10">
                    <p>Together with Shixiong Zhang, Yong Zhao, Jinyu Li and Yifan Gong, we created a end to end speaker verfication model, which is the first deep learning based model that can be used for both text dependent and text independent speaker verification. </p>
                </div>

                <h3 class="col-md-10">Elevox </h3>
                <p class="col-md-2" id="year">2015</p>     
                <div class="col-md-10">
                    <p>Together with sevaral friends of mine, I found <b><i><a href="http://www.elevoc.com/" target="_blank"> this </a></i></b> company, specically for speech enhancement and audio source separation. I left this company in 2016 </p>
                </div>

                <h3 class="col-md-10">Deep clustering </h3>
                <p class="col-md-2" id="year">2015</p>     
                <div class="col-md-10">
                    <p>Together with John Hershey, Jonathan LeRoux, Shinji Watanabe and Yusuf Isik, we invented this revolutionary technic. We refresh the previous state of art performance by THREE TIMES. And it is the first time for human to achieve the high quality for overlapped unknown speaker (and unknown number of speaker) separation. See demo <b><i><a href="http://www.merl.com/demos/deep-clustering" target="_blank"> here  </a></i></b> </p>
                </div>

                <h3 class="col-md-10">The 3rd Chime challenge </h3>
                <p class="col-md-2" id="year">2015</p>     
                <div class="col-md-10">
                    <p>Together with researchers in MERL and SRI, we got the 2nd best performance in the 3rd CHiME challenge, a world level challenge for automatic speech recognition under noisy enviorment </p>
                </div>

                <h3 class="col-md-10">The IRAPA-ASpIRE challenge </h3>
                <p class="col-md-2" id="year">2015</p>     
                <div class="col-md-10">
                    <p>Together with researcher in MERL and BBN, we won the IRAPA-ASpIRE challenge, a world level ASR evalutation under highly corrupted enviroment  </p>
                </div>

            </div>
        </div>
    </section>

    <section id="education" class="education-section">
        <div class="container">
            <h1>EDUCATION</h1>
            <div class="row">
                <h3 class="col-md-10">Kyushu University</h3>
                <p class="col-md-2" id="year">2011-2015</p>
                <div class="col-md-10">
                    <p id="title">PhD in Computer Science, Laboratory for Image and Media Understanding (LIMU)</p>
                    <p>Doctor thesis: Light Field Vision for Transparent Object Recognition.</p>
                </div>
                    
                <h3 class="col-md-10">University of Chinese Academy of Sciences</h3>
                <p class="col-md-2" id="year">2007-2010</p>     
                <div class="col-md-10">
                    <p id="title">M.S. in Signal and Information Processing, Shanghai Institute of Applied Physics (SINAP)</p>
                    <p>Master thesis: Beam Profile Monitor System Based on Industrial Ethernet.</p>
                </div>
                    
                <h3 class="col-md-10">Beijing Electronic Science and Technology Institute</h3>
                <p class="col-md-2" id="year">2003-2007</p>
                <div class="col-md-10">
                    <p id="title">B.S. in Communication Engineering</p>
                    <p>Bachelor thesis: Improvement of Mobile Communication Experimental Instrument.</p>
                </div>
            </div>
        </div>
    </section>


    <section id="work" class="work-section">
        <div class="container">
            <h1>WORK EXPERIENCE</h1>
            <div class="row">
                <h3 class="col-md-10">Microsoft</h3>
                <p class="col-md-2" id="year">06/2016-09/2016</p>   
                <div class="col-md-10">
                    <p id="title">Research Intern | Bellevue, USA</p>
                    <ul>
                        <li>Focused on single and mulit-channel speech recognition in far-field and mis-matched condition.</li>
                        <li>Focused on text-independent end to end speaker verification system with attentional neural network</li>
                    </ul>
                </div>

                <h3 class="col-md-10">Jelenik Speech and Language Technologies Workshop</h3>
                <p class="col-md-2" id="year">07/2015-09/2015</p>   
                <div class="col-md-10">
                    <p id="title">FFS Team Member | Seattle, USA</p>
                    <ul>
                        <li>Focused on the speech recognition in far-field and mis-matched condition.</li>
                        <li>Multi-channel auditory source separation using the recurrennt neural network.</li>
                    </ul>
                </div>

                <h3 class="col-md-10">Mitsubishi Electric Research Laboratories</h3>
                <p class="col-md-2" id="year">11/2014-06/2015</p>        
                <div class="col-md-10">
                    <p id="title">Intern Researcher | Boston, USA</p>
                    <ul>
                        <li>Robust automatic speech recognition and enhancement in heavily noisy and mis-matched condition.</li>
                        <li>Deep learning with Long Short-term Memory neural network with application in the auditory source separation.</li>
                        <li>Sequence tmbedding with deep neural network with application in speaker recognition.</li>
                    </ul>
                </div>

                <h3 class="col-md-10">International Computer Science Institute</h3>
                <p class="col-md-2" id="year">06/2013-06/2014</p>
                <div class="col-md-10">
                    <p id="title">Research assistant | Berkeley, USA</p>
                    <ul>
                        <li>Robust front end for multi-language automatic speech recognition.</li>
                        <li>Robust decomposition based speech enhancement system.</li>
                        <li>Automatic speech recognition for minor languages.</li>
                    </ul>
                </div>
            </div>
        </div>
    </section>



    <section id="patent" class="patent-section">
        <div class="container">
            <div class="row">
                <div class="col-md-12">
                    <h1 id="header">PATENT</h1>
                    <ol class="publication_list">
                        <li>Mesgarani, N., O’Sullivan, J., Chen, Zhuo <b><i>“Neural decoding of attentional selection in multi-speaker environments without access to separated sources”</i></b>,  Provisional Patent filed June 2016.</li>
                        <li>John Hershey, Jonathan Le Roux, Shinji Watanabe, Zhuo Chen <b><i>“Method for distinguishing components of an acoustic signal”</i></b>, US patent No. 9368110, 2016</li>
                    </ol>
                </div>
            </div>
        </div>
    </section>
    <!-- Publication Section -->
    <section id="publication" class="publication-section">
        <div class="container">
            <div class="row">
                <div class="col-md-12">
                    <h1 id="header">PUBLICATION</h1>
                    <ol class="publication_list">
                        <li>Yi Luo, Zhuo Chen, Jonathan Le Roux, John Hershey, Nima Mesgarani, <b><i>“Deep Clustering andConventional Networks for Music Separation: Stronger Together”</i></b>, submitted to ICASSP 2017.</li>
                        <li>Zhuo Chen, Yan Huang, Jinyu Li, Yifan Gong, <b><i>“Improving mask learning based speech enhancement system with restoration layers and residual connection”</i></b>, submitted to ICASSP 2017.</li>
                        <li>Zhuo Chen, Yi luo, Nima Mesgarani, <b><i>“Deep attractor network for single-microphone speaker separation”</i></b>, submitted to ICASSP 2017.</li>
                        <li>Yi Luo, Zhuo Chen, Jonathan Le Roux, John Hershey, Daniel P.W Ellis, <b><i>““Deep Clustering For Singing Voice Separation”</i></b>, MIREX, task ofSinging Voice Separation, 2016(1st and 2nd performance).</li>
                        <li>Shi-Xiong Zhang, Zhuo Chen, Yong Zhao, Jinyu Li, Yifan Gong, <b><i>“End-to-End Attention based Text-Dependent Speaker Verification”</i></b>, in 2016 IEEE Workshop on Spoken Language Technology, Dec 2016.</li>
                        <li>Yusuf Isik, Jonathan Le Roux, Zhuo Chen, Shinji Watanabe, John R. Hershey, <b><i>“Single-Channel Multi-Speaker Separation Using Deep Clustering”</i></b>, in Proc. Interspeech, San Francisco, Sep 2016.</li>
                        <li>Tasha Negamine,Zhuo Chen, Nima Mesgarani, <b><i>“Adaptation of Neural Networks Constrained by Prior Statistics of Node Co-Activations”</i></b>, in Proc. Interspeech, San Francisco, Sep 2016</li>
                        <li>Z. Chen, J. O'Sullivan, S. Sheth, G. Mckann, A. D. Mehta, N. Mesgarani, <b><i>“Neural decoding of attentional selection in multi-speaker environments without access to separated sources”</i></b>, in Society for Neuroscience 2016, San Diego, Nov 2016.</li>
                        <li>John R. Hershey, Zhuo Chen, Jonathan Le Roux, Shinji Watanabe, Yusuf Isik, <b><i>“Deep clustering:Discriminative embeddings for segmentation and separation”</i></b>, in Proc.ICASSP, Shanghai, April 2016.</li>
                        <li>T. Hori, Z. Chen, H. Erdogan, J. Hershey, J. Roux, V. Mitra, S. Watanabe, <b><i>“The Merl/sri System For The 3rd Chime Challenge Using Beamforming, Robust Feature Extraction, And Advanced Speech Recognition”</i></b>, in Proc. ASRU, Arizona, Dec 2015.</li>
                        <li>R. Hsiao, J. Ma, W. Hartmann, M. Karafiat, F. Grezl, L. Burget, I. Szoke, J.H. Cernocky, S. Watanabe, Z. Chen, S. Mallidi, H. Hermansky, S. Tsakalidis, R. Schwartz, <b><i>“Robust Speech Recognition in Unknown Reverberant and Noisy Conditions”</i></b>, in Proc. ASRU, Arizona, Dec 2015.</li>             
                        <li>Z. Chen, S. Watanabe H. Erdogan, J. Hershey, <b><i><a href="http://www.merl.com/publications/docs/TR2015-100.pdf" target="_blank">“Speech enhancement and recognition using multi-task learning of long short-term memory recurrent neural networks”</a></i></b>,in Proc. Interspeech, Dresden, Sep 2015.</li>
                        <li>Z. Chen, B. McFee, D. Ellis, <b><i><a href="https://www.ee.columbia.edu/~dpwe/pubs/ChenME14-cdl.pdf" target="_blank">“Speech enhancement by low-rank and convolutive dictionary spectrogram decomposition”</a></i></b>, in Proc. Interspeech, Singapore, Sep 2014.</li>
                        <li>D. Ellis and H. Satoh and Z. Chen, <b><i><a href="https://www.ee.columbia.edu/~dpwe/pubs/EllisSC14-proximity.pdf" target="_blank">“Detecting proximity from personal audio recordings”</a></i></b>, inProc. Interspeech, Singapore, Sep 2014.</li>
                        <li>Z. Chen, H. Papadopoulos, D. Ellis, <b><i><a href="https://www.ee.columbia.edu/~dpwe/pubs/ChenPE14-splr.pdf" target="_blank">“Content-adaptive speech enhancement by a sparsely-activated dictionary plus low rank decomposition”</a></i></b>, in Proc. HSCMA, Nancy, May 2014.</li>
                        <li>Z. Chen, D. Ellis, <b><i><a href="https://www.ee.columbia.edu/~dpwe/pubs/ChenE13-sparse" target="_blank">“Speech enhancement by sparse, low-rank, and dictionary spectrogram decomposition”</a></i></b>, in Proc. 2013, Workshop on Applications of Signal Processing to Audio and Acoustics, New Paltz, NY, 2013.</li>
                        <li>Z. Chen, G. Grindlay, D. Ellis, <b><i>“Transcribing multi-instrument polyphonic music with transformed eigeninstrument whole- note templates”</i></b>, MIREX, task of Multiple Fundamental Frequency Estimation and Tracking , 2012.</li>
                    </ol>
                </div>
            </div>
        </div>
    </section>
    
    <!-- footer Section -->
    <footer id="footer">
        <p>Copyright © Yichao Xu</p>
        
    </footer>


    <!-- jQuery -->
    <script src="js/jquery.js"></script>

    <!-- Bootstrap Core JavaScript -->
    <script src="js/bootstrap.min.js"></script>

    <!-- Scrolling Nav JavaScript -->
    <script src="js/jquery.easing.min.js"></script>
    <script src="js/scrolling-nav.js"></script>





</body></html>